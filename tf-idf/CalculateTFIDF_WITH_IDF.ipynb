{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.text import TextCollection\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import Preprocessor\n",
    "from bson import ObjectId\n",
    "import operator\n",
    "import sklearn.metrics.pairwise as sklearnPairwise\n",
    "from scipy.sparse import hstack,vstack\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got  37821  Documents\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"localhost\")\n",
    "db=client.crawlerdb_WORK_TFIDF_3\n",
    "collection = db.crawlerdb\n",
    "result_documents_list_REAL = list(collection.find({})) # work-data-collection\n",
    "print(\"We got \", len(result_documents_list_REAL), \" Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor.Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We still have  37821  Documents\n"
     ]
    }
   ],
   "source": [
    "#result_document_text_list = [a[\"text\"] for a in result_documents_list_REAL] # wir müssen erst result_documents_list[\"text\"] \"entpacken\" von cursor\n",
    "result_document_text_list = list()\n",
    "for i in range(len(result_documents_list_REAL)):\n",
    "    try:\n",
    "        result_document_text_list.append(result_documents_list_REAL[i][\"text\"]) # Fehler: KeyError: muss abgefangen werden\n",
    "    except KeyError:\n",
    "        pass\n",
    "print(\"We still have \", len(result_document_text_list), \" Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_values = tfidf.fit_transform(result_document_text_list) # output= <class 'scipy.sparse.csr.csr_matrix'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erstellt ein Dict mit {idOfDocument(andNotMore): tf-idf-value}, wobei tf-idf-value eine sparse matrix mit 1xanzahlUniquerWörter ist\n",
    "result_dict_with_ids = {}\n",
    "for i in range(len(result_documents_list_REAL)):\n",
    "    key = str(result_documents_list_REAL[i][\"_id\"]) # 5c2fb3c988df422822370767\n",
    "    result_dict_with_ids[key] = tfidf_values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_with_ids[\"5c2fb3c988df422822370767\"] # with 65 stored elements in Compressed Sparse Row format> means that 65 times is not a 0\n",
    "shape_of_one_document = result_dict_with_ids[\"5c2fb3c988df422822370767\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finished pre TF_IDF - Calculation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['haus']\n",
      "CPU times: user 3.76 ms, sys: 266 µs, total: 4.02 ms\n",
      "Wall time: 5.36 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "search_term = \"haus\"\n",
    "tokenized_words = preprocessor.tokenizing_without_punc(search_term)\n",
    "stemmed_search_term = preprocessor.stemming_words(tokenized_words)\n",
    "\n",
    "tfidf_search_term = tfidf.transform([search_term]) # jetzt brauchen wir ja wieder den \"richtigen\" searchterm zum Vergleich # transform passt idf nicht an\n",
    "\n",
    "\n",
    "print(stemmed_search_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.3 ms, sys: 0 ns, total: 3.3 ms\n",
      "Wall time: 19.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "collection_inverse_index = db.crawlerdb_INVERSE_INDEX\n",
    "test1 = collection_inverse_index.find({\"word\": {\"$in\": tokenized_words}})\n",
    "result_ids = set()\n",
    "for i in test1:\n",
    "    result_ids.update(i[\"documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in production file we use union atm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # combine result ids from inverse index with in a set via intersection\n",
    "# #collection_inverse_index = db.crawlerdb_INVERSE_INDEX\n",
    "# result_list_from_inverse_index = db.crawlerdb_INVERSE_INDEX.find({\"word\": {\"$in\": tokenized_words}}) ######################################################### tokenized und nicht stemmed\n",
    "# try:\n",
    "#     result_ids = set(result_list_from_inverse_index[0][\"documents\"])\n",
    "#     for i in result_list_from_inverse_index:\n",
    "#         result_ids = result_ids.intersection(i[\"documents\"])\n",
    "# except IndexError:\n",
    "#     result_ids = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### holt sich die tf-idf werte zu den ids aus den inversen index (result_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 ms, sys: 3.94 ms, total: 18.3 ms\n",
      "Wall time: 17.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "less_document_actually= matrix = vstack([result_dict_with_ids[i] for i in result_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get from our TF-IDF - Vector the given important documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.19 ms, sys: 12 µs, total: 4.2 ms\n",
      "Wall time: 3.09 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cos_sim_results = sklearnPairwise.cosine_similarity(X=matrix,Y=tfidf_search_term).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 399 µs, sys: 18 µs, total: 417 µs\n",
      "Wall time: 422 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_ids_with_scores = {}\n",
    "i = 0\n",
    "for a in result_ids:\n",
    "    new_ids_with_scores[a] = cos_sim_results[i]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('5c30582988df4228e7867dd1', 0.49931165727219057), ('5c303e1c88df4228e7866bea', 0.3654545667638543), ('5c30128388df4228e7864deb', 0.3538282196534258), ('5c2ff4d088df4228e78639ee', 0.33912854975358947), ('5c303fd688df4228e7866d1c', 0.334818161485579), ('5c2fd41d88df4228e78623b6', 0.3293500593034371), ('5c3065b988df4228e78686e1', 0.31176762757845133), ('5c303e4688df4228e7866c08', 0.308486248326046), ('5c30207188df4228e786579e', 0.29276529806948226), ('5c30032d88df4228e7864393', 0.2917058867815179), ('5c2fc5b788df4228e7861a18', 0.28527732688503293), ('5c2fb3e888df422822371391', 0.2841450887408819), ('5c2ff32488df4228e78638d4', 0.2838054182836621), ('5c30585088df4228e7867de8', 0.2813745314582979), ('5c2fb40c88df422822372229', 0.2737912371506526), ('5c30462188df4228e7867177', 0.27212007492967977), ('5c303ee388df4228e7866c71', 0.2700960584205649), ('5c2fb3e888df4228223713e7', 0.26770139023395), ('5c3042bf88df4228e7866f1d', 0.26749673241313315), ('5c302fc988df4228e786622a', 0.26461821654776124)]\n",
      "CPU times: user 1.31 ms, sys: 0 ns, total: 1.31 ms\n",
      "Wall time: 1.46 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sorted_d = sorted(new_ids_with_scores.items(), key=operator.itemgetter(1), reverse=True)[0:20]\n",
    "print(sorted_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 µs, sys: 1e+03 ns, total: 31 µs\n",
      "Wall time: 33.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ids = [ObjectId(sorted_d[i][0]) for i in range(len(sorted_d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "CPU times: user 1.83 ms, sys: 82 µs, total: 1.91 ms\n",
      "Wall time: 2.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents = collection.find({\"_id\" : {\"$in\" : ids}})\n",
    "print(type(documents[0]))\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 796 µs, sys: 16 µs, total: 812 µs\n",
      "Wall time: 1.25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_of_documents = list(documents)\n",
    "for i in range(len(list_of_documents)):\n",
    "    list_of_documents[i][\"cossim\"] = sorted_d[i][1]\n",
    "list_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
