{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.text import TextCollection\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import Preprocessor\n",
    "from bson import ObjectId\n",
    "import operator\n",
    "import sklearn.metrics.pairwise as sklearnPairwise\n",
    "from scipy.sparse import hstack,vstack\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got  37821  Documents\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"localhost\")\n",
    "db=client.crawlerdb_WORK_TFIDF_3\n",
    "collection = db.crawlerdb\n",
    "result_documents_list_REAL = list(collection.find({})) # work-data-collection\n",
    "print(\"We got \", len(result_documents_list_REAL), \" Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We still have  37821  Documents\n"
     ]
    }
   ],
   "source": [
    "#result_document_text_list = [a[\"text\"] for a in result_documents_list_REAL] # wir müssen erst result_documents_list[\"text\"] \"entpacken\" von cursor\n",
    "result_document_text_list = list()\n",
    "for i in range(len(result_documents_list_REAL)):\n",
    "    try:\n",
    "        result_document_text_list.append(result_documents_list_REAL[i][\"text\"]) # Fehler: KeyError: muss abgefangen werden\n",
    "    except KeyError:\n",
    "        pass\n",
    "print(\"We still have \", len(result_document_text_list), \" Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_values = tfidf.fit_transform(result_document_text_list) # output= <class 'scipy.sparse.csr.csr_matrix'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erstellt ein Dict mit {idOfDocument(andNotMore): tf-idf-value}, wobei tf-idf-value eine sparse matrix mit 1xanzahlUniquerWörter ist\n",
    "result_dict_with_ids = {}\n",
    "for i in range(len(result_documents_list_REAL)):\n",
    "    key = str(result_documents_list_REAL[i][\"_id\"]) # 5c2fb3c988df422822370767\n",
    "    result_dict_with_ids[key] = tfidf_values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_with_ids[\"5c2fb3c988df422822370767\"] # with 65 stored elements in Compressed Sparse Row format> means that 65 times is not a 0\n",
    "shape_of_one_document = result_dict_with_ids[\"5c2fb3c988df422822370767\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finished pre TF_IDF - Calculation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frau']\n",
      "CPU times: user 444 ms, sys: 56.1 ms, total: 500 ms\n",
      "Wall time: 498 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "search_term = \"Frau\"\n",
    "\n",
    "preprocessor = Preprocessor.Preprocessor()\n",
    "tokenized_words = preprocessor.tokenizing_without_punc(search_term)\n",
    "stemmed_search_term = preprocessor.stemming_words(tokenized_words)\n",
    "\n",
    "tfidf_search_term = tfidf.transform([search_term]) # jetzt brauchen wir ja wieder den \"richtigen\" searchterm zum Vergleich # transform passt idf nicht an\n",
    "\n",
    "\n",
    "print(stemmed_search_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# collection_inverse_index = db.crawlerdb_INVERSE_INDEX\n",
    "# test1 = collection_inverse_index.find({\"word\": {\"$in\": tokenized_words}})\n",
    "# result_ids = set()\n",
    "# for i in test1:\n",
    "#     result_ids.update(i[\"documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in production file we use union atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine result ids from inverse index with in a set via intersection\n",
    "#collection_inverse_index = db.crawlerdb_INVERSE_INDEX\n",
    "result_list_from_inverse_index = db.crawlerdb_INVERSE_INDEX.find({\"word\": {\"$in\": tokenized_words}}) ######################################################### tokenized und nicht stemmed\n",
    "try:\n",
    "    result_ids = set(result_list_from_inverse_index[0][\"documents\"])\n",
    "    for i in result_list_from_inverse_index:\n",
    "        result_ids = result_ids.intersection(i[\"documents\"])\n",
    "except IndexError:\n",
    "    result_ids = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### holt sich die tf-idf werte zu den ids aus den inversen index (result_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.2 ms, sys: 4 µs, total: 44.2 ms\n",
      "Wall time: 43.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "less_document_actually= matrix = vstack([result_dict_with_ids[i] for i in result_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get from our TF-IDF - Vector the given important documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 ms, sys: 96 µs, total: 14.2 ms\n",
      "Wall time: 8.22 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cos_sim_results = sklearnPairwise.cosine_similarity(X=matrix,Y=tfidf_search_term).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.12 ms, sys: 0 ns, total: 2.12 ms\n",
      "Wall time: 2.14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_ids_with_scores = {}\n",
    "i = 0\n",
    "for a in result_ids:\n",
    "    new_ids_with_scores[a] = cos_sim_results[i]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('5c30560c88df4228e7867c5d', 0.36574673771819827), ('5c304c5388df4228e78675ab', 0.35227582435405413), ('5c30448088df4228e7867055', 0.33555684692842946), ('5c2fb41988df422822372741', 0.3332914614070332), ('5c2ffac888df4228e7863df9', 0.3302862631240846), ('5c2fe00588df4228e7862bc3', 0.32469365555761287), ('5c30119088df4228e7864d44', 0.3246037234150033), ('5c301ec788df4228e786566d', 0.31953061799799753), ('5c303b7e88df4228e7866a21', 0.3106536428519034), ('5c301eaf88df4228e786565b', 0.30926063744355803), ('5c302c6488df4228e7865fd4', 0.30850192822376354), ('5c2fb3eb88df4228223714f5', 0.30610220075306793), ('5c2fcf3a88df4228e7862078', 0.3060917138545589), ('5c30133088df4228e7864e65', 0.304665108422399), ('5c303eb288df4228e7866c4d', 0.3039727975919692), ('5c2fdf9c88df4228e7862b7a', 0.29724271469165364), ('5c30177088df4228e7865152', 0.29488329785888295), ('5c2fdb9c88df4228e78628be', 0.29332635693050974), ('5c2fb3dc88df422822370f0b', 0.29301176051425165), ('5c2ff6cd88df4228e7863b43', 0.2928333056228763)]\n",
      "CPU times: user 46 ms, sys: 12 µs, total: 46 ms\n",
      "Wall time: 26.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sorted_d = sorted(new_ids_with_scores.items(), key=operator.itemgetter(1), reverse=True)[0:20]\n",
    "print(sorted_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 µs, sys: 0 ns, total: 52 µs\n",
      "Wall time: 55.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ids = [ObjectId(sorted_d[i][0]) for i in range(len(sorted_d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "CPU times: user 6.57 ms, sys: 0 ns, total: 6.57 ms\n",
      "Wall time: 2.83 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents = collection.find({\"_id\" : {\"$in\" : ids}})\n",
    "print(type(documents[0]))\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 785 µs, sys: 36 µs, total: 821 µs\n",
      "Wall time: 1.03 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_of_documents = list(documents)\n",
    "for i in range(len(list_of_documents)):\n",
    "    list_of_documents[i][\"cossim\"] = sorted_d[i][1]\n",
    "list_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
